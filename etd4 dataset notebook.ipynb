{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 297915 CSV files. Processing 15% of them...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44687/44687 [6:14:13<00:00,  1.99it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Processing Complete! Saved to: C:\\Users\\palak\\Desktop\\sem6\\Intel_unaati\\zipped_processed_student_data_sampled.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# ‚úÖ Define Paths\n",
    "zip_file_path = r\"C:\\Users\\palak\\Desktop\\sem6\\Intel_unaati\\EdNet-KT4.zip\"\n",
    "output_file = r\"C:\\Users\\palak\\Desktop\\sem6\\Intel_unaati\\zipped_processed_student_data_sampled.csv\"\n",
    "\n",
    "# ‚úÖ Initialize an empty DataFrame\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# ‚úÖ Open ZIP file without extracting\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
    "    file_list = [f for f in z.namelist() if f.endswith('.csv')]  # Filter only CSV files\n",
    "\n",
    "    print(f\"‚úÖ Found {len(file_list)} CSV files. Processing 15% of them...\")\n",
    "\n",
    "    # Sample 25% of the files\n",
    "    sample_files = pd.Series(file_list).sample(frac=0.15, random_state=42).tolist()\n",
    "\n",
    "    for file in tqdm(sample_files, desc=\"Processing Files\"):\n",
    "        try:\n",
    "            # Read CSV directly from ZIP without extracting\n",
    "            with z.open(file) as f:\n",
    "                df = pd.read_csv(f)\n",
    "\n",
    "                # Convert timestamp column if it exists\n",
    "                if 'timestamp' in df.columns:\n",
    "                    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "\n",
    "                # Append to the combined dataframe\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping file {file} due to error: {e}\")\n",
    "\n",
    "# ‚úÖ Save the combined dataset\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ Data Processing Complete! Saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Loaded Successfully!\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19077465 entries, 0 to 19077464\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   timestamp    object \n",
      " 1   action_type  object \n",
      " 2   item_id      object \n",
      " 3   cursor_time  float64\n",
      " 4   source       object \n",
      " 5   user_answer  object \n",
      " 6   platform     object \n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 1018.8+ MB\n",
      "\n",
      "üìä Data Info:\n",
      " None\n",
      "\n",
      "üîç Sample Data:\n",
      "                  timestamp action_type item_id  cursor_time     source  \\\n",
      "0  2018-09-14 01:36:22.395       enter   b5060          NaN  diagnosis   \n",
      "1  2018-09-14 01:36:55.909     respond   q6528          NaN  diagnosis   \n",
      "2  2018-09-14 01:36:56.645      submit   b5060          NaN  diagnosis   \n",
      "3  2018-09-14 01:36:58.168       enter   b3779          NaN  diagnosis   \n",
      "4  2018-09-14 01:37:21.381     respond   q5247          NaN  diagnosis   \n",
      "\n",
      "  user_answer platform  \n",
      "0         NaN      web  \n",
      "1           d      web  \n",
      "2         NaN      web  \n",
      "3         NaN      web  \n",
      "4           b      web  \n",
      "\n",
      "‚ùå Missing Values:\n",
      " timestamp             0\n",
      "action_type           0\n",
      "item_id               0\n",
      "cursor_time    13731578\n",
      "source             4260\n",
      "user_answer    14982799\n",
      "platform           4260\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Load Processed Data\n",
    "file_path = r\"C:\\Users\\palak\\Desktop\\sem6\\Intel_unaati\\zipped_processed_student_data_sampled.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ‚úÖ Display Data Info\n",
    "print(\"‚úÖ Data Loaded Successfully!\")\n",
    "print(\"\\nüìä Data Info:\\n\", df.info())\n",
    "print(\"\\nüîç Sample Data:\\n\", df.head())\n",
    "\n",
    "print(\"\\n‚ùå Missing Values:\\n\", df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for numerical columns\n",
    "df.fillna(method='ffill', inplace=True)  # Forward fill\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime and extract features\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.hour  # Extract hour\n",
    "df['day'] = df['timestamp'].dt.day  # Extract day\n",
    "df['month'] = df['timestamp'].dt.month  # Extract month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical features\n",
    "df = pd.get_dummies(df, columns=['action_type', 'source', 'platform'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train LSTM for Student Engagement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 timestamp action_type item_id  cursor_time     source  \\\n",
      "0  2018-09-14 01:36:22.395       enter   b5060          NaN  diagnosis   \n",
      "1  2018-09-14 01:36:55.909     respond   q6528          NaN  diagnosis   \n",
      "2  2018-09-14 01:36:56.645      submit   b5060          NaN  diagnosis   \n",
      "3  2018-09-14 01:36:58.168       enter   b3779          NaN  diagnosis   \n",
      "4  2018-09-14 01:37:21.381     respond   q5247          NaN  diagnosis   \n",
      "\n",
      "  user_answer platform  \n",
      "0         NaN      web  \n",
      "1           d      web  \n",
      "2         NaN      web  \n",
      "3         NaN      web  \n",
      "4           b      web  \n",
      "Index(['timestamp', 'action_type', 'item_id', 'cursor_time', 'source',\n",
      "       'user_answer', 'platform'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\palak\\Desktop\\sem6\\Intel_unaati\\zipped_processed_student_data_sampled.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display first few rows to understand its structure\n",
    "print(df.head())\n",
    "\n",
    "# Display column names to ensure all features are present\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['action_type', 'item_id', 'cursor_time', 'source', 'user_answer',\n",
      "       'platform', 'hour', 'day', 'month'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check the actual columns in the DataFrame\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19077465 entries, 0 to 19077464\n",
      "Data columns (total 26 columns):\n",
      " #   Column                         Dtype  \n",
      "---  ------                         -----  \n",
      " 0   item_id                        object \n",
      " 1   cursor_time                    float64\n",
      " 2   user_answer                    object \n",
      " 3   hour                           int32  \n",
      " 4   day                            int32  \n",
      " 5   month                          int32  \n",
      " 6   action_type_enter              bool   \n",
      " 7   action_type_erase_choice       bool   \n",
      " 8   action_type_pause_audio        bool   \n",
      " 9   action_type_pause_video        bool   \n",
      " 10  action_type_pay                bool   \n",
      " 11  action_type_play_audio         bool   \n",
      " 12  action_type_play_video         bool   \n",
      " 13  action_type_quit               bool   \n",
      " 14  action_type_refund             bool   \n",
      " 15  action_type_respond            bool   \n",
      " 16  action_type_submit             bool   \n",
      " 17  action_type_undo_erase_choice  bool   \n",
      " 18  source_archive                 bool   \n",
      " 19  source_diagnosis               bool   \n",
      " 20  source_my_note                 bool   \n",
      " 21  source_review                  bool   \n",
      " 22  source_review_quiz             bool   \n",
      " 23  source_sprint                  bool   \n",
      " 24  source_tutor                   bool   \n",
      " 25  platform_web                   bool   \n",
      "dtypes: bool(20), float64(1), int32(3), object(2)\n",
      "memory usage: 1018.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\palak\\Desktop\\sem6\\Intel_unaati\\zipped_processed_student_data_sampled.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'timestamp' column to datetime and extract features\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['day'] = df['timestamp'].dt.day\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "\n",
    "# Drop 'timestamp' column as it's no longer needed\n",
    "df.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "# One-hot encode categorical features (action_type, source, platform)\n",
    "df = pd.get_dummies(df, columns=['action_type', 'source', 'platform'], drop_first=True)\n",
    "\n",
    "# Fill missing values by forward filling\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Check the data after preprocessing\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\palak\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:480: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "c:\\Users\\palak\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:481: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15261972, 1, 24) (3815493, 1, 24)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the chunk size (adjust as needed)\n",
    "chunk_size = 10000  # You can reduce this if memory is still an issue\n",
    "\n",
    "# Initialize a list to store processed chunks\n",
    "chunks = []\n",
    "\n",
    "# Initialize LabelEncoder for user_answer\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Iterate over the CSV file in chunks\n",
    "for chunk in pd.read_csv(r\"C:\\Users\\palak\\Desktop\\sem6\\Intel_unaati\\zipped_processed_student_data_sampled.csv\", \n",
    "                         chunksize=chunk_size):\n",
    "    \n",
    "    # Process each chunk: Drop 'item_id' column\n",
    "    chunk.drop(columns=['item_id'], inplace=True)\n",
    "    \n",
    "    # Convert 'user_answer' to numeric labels\n",
    "    chunk['user_answer'] = label_encoder.fit_transform(chunk['user_answer'])\n",
    "    \n",
    "    # Extract time-related features from 'timestamp'\n",
    "    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])\n",
    "    chunk['hour'] = chunk['timestamp'].dt.hour\n",
    "    chunk['day'] = chunk['timestamp'].dt.day\n",
    "    chunk['month'] = chunk['timestamp'].dt.month\n",
    "    \n",
    "    # Drop 'timestamp' as it's no longer needed\n",
    "    chunk.drop(columns=['timestamp'], inplace=True)\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    chunk = pd.get_dummies(chunk, columns=['action_type', 'source', 'platform'], drop_first=True)\n",
    "    \n",
    "    # Fill missing values by forward filling\n",
    "    chunk.fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    # Scale the features\n",
    "    X = chunk.drop(columns=['user_answer'])\n",
    "    y = chunk['user_answer']\n",
    "    X_scaled = scaler.fit_transform(X)  # Scale the chunk\n",
    "\n",
    "    # Add the scaled chunk to the list\n",
    "    chunks.append((X_scaled, y))\n",
    "\n",
    "# Combine all the chunks\n",
    "X_combined = pd.concat([pd.DataFrame(chunk[0]) for chunk in chunks], axis=0)\n",
    "y_combined = pd.concat([pd.Series(chunk[1]) for chunk in chunks], axis=0)\n",
    "\n",
    "# Reshape for LSTM (samples, time_steps, features)\n",
    "X_lstm = X_combined.values.reshape((X_combined.shape[0], 1, X_combined.shape[1]))  # Adding a time step dimension\n",
    "\n",
    "# Split into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lstm, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of the data\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15261972, 1, 24) (3815493, 1, 24)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define chunk size for memory efficiency\n",
    "chunk_size = 100000\n",
    "\n",
    "# Initialize a list to store processed chunks\n",
    "chunks = []\n",
    "\n",
    "# Initialize LabelEncoder and MinMaxScaler\n",
    "label_encoder = LabelEncoder()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Iterate over the CSV file in chunks\n",
    "for chunk in pd.read_csv(r\"C:\\Users\\palak\\Desktop\\sem6\\Intel_unaati\\zipped_processed_student_data_sampled.csv\", \n",
    "                         chunksize=chunk_size):\n",
    "    \n",
    "    # Drop 'item_id' column as it's not needed for training\n",
    "    chunk.drop(columns=['item_id'], inplace=True)\n",
    "    \n",
    "    # Convert 'user_answer' to numeric labels\n",
    "    chunk['user_answer'] = label_encoder.fit_transform(chunk['user_answer'])\n",
    "    \n",
    "    # Extract time-related features from 'timestamp'\n",
    "    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])\n",
    "    chunk['hour'] = chunk['timestamp'].dt.hour\n",
    "    chunk['day'] = chunk['timestamp'].dt.day\n",
    "    chunk['month'] = chunk['timestamp'].dt.month\n",
    "    \n",
    "    # Drop 'timestamp' as it's no longer needed\n",
    "    chunk.drop(columns=['timestamp'], inplace=True)\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    chunk = pd.get_dummies(chunk, columns=['action_type', 'source', 'platform'], drop_first=True)\n",
    "    \n",
    "    # Fill missing values by forward filling\n",
    "    chunk.fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    # Remove columns that are completely NaN (if any)\n",
    "    chunk = chunk.dropna(axis=1, how='all')\n",
    "    \n",
    "    # Scale the features\n",
    "    X = chunk.drop(columns=['user_answer'])\n",
    "    y = chunk['user_answer']\n",
    "    X_scaled = scaler.fit_transform(X)  # Scale the chunk\n",
    "\n",
    "    # Add the scaled chunk to the list\n",
    "    chunks.append((X_scaled, y))\n",
    "\n",
    "# Combine all the chunks\n",
    "X_combined = pd.concat([pd.DataFrame(chunk[0]) for chunk in chunks], axis=0)\n",
    "y_combined = pd.concat([pd.Series(chunk[1]) for chunk in chunks], axis=0)\n",
    "\n",
    "# Reshape for LSTM (samples, time_steps, features)\n",
    "X_lstm = X_combined.values.reshape((X_combined.shape[0], 1, X_combined.shape[1]))  # Adding a time step dimension\n",
    "\n",
    "# Split into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lstm, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of the data\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 1, 64)             22784     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1, 64)             0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35299 (137.89 KB)\n",
      "Trainable params: 35299 (137.89 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),  # Shape (samples, time_steps, features)\n",
    "    Dropout(0.2),  # Dropout to prevent overfitting\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(3, activation='softmax')  # 3 classes: 'user_answer_b', 'user_answer_c', 'user_answer_d'\n",
    "])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'user_answer': [4 3 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Check the unique values in 'user_answer'\n",
    "print(\"Unique values in 'user_answer':\", y_combined.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values after re-encoding: [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Re-encode 'user_answer' to correctly reflect 5 classes\n",
    "label_encoder = LabelEncoder()\n",
    "y_combined = label_encoder.fit_transform(y_combined)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Check the unique values after re-encoding\n",
    "print(\"Unique values after re-encoding:\", np.unique(y_combined))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 1, 64)             22784     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1, 64)             0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35365 (138.14 KB)\n",
      "Trainable params: 35365 (138.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Rebuild the LSTM model with correct number of output units\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),  # Shape (samples, time_steps, features)\n",
    "    Dropout(0.2),  # Dropout to prevent overfitting\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(5, activation='softmax')  # Update to 5 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary to confirm changes\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaN or Inf in X_train:\n",
      "16384934 0\n",
      "Checking for NaN or Inf in y_train:\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any NaN or infinite values in the dataset\n",
    "import numpy as np\n",
    "\n",
    "print(\"Checking for NaN or Inf in X_train:\")\n",
    "print(np.isnan(X_train).sum(), np.isinf(X_train).sum())\n",
    "\n",
    "print(\"Checking for NaN or Inf in y_train:\")\n",
    "print(np.isnan(y_train).sum(), np.isinf(y_train).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaN in X_train after filling:\n",
      "0 0\n",
      "Checking for NaN in X_test after filling:\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define chunk size (adjust based on available memory)\n",
    "chunk_size = 100000  # Adjust the chunk size\n",
    "\n",
    "# Initialize empty lists to store processed chunks\n",
    "X_train_chunks = []\n",
    "X_test_chunks = []\n",
    "\n",
    "# Process X_train in chunks\n",
    "for start in range(0, X_train.shape[0], chunk_size):\n",
    "    end = min(start + chunk_size, X_train.shape[0])\n",
    "    \n",
    "    chunk = X_train[start:end]\n",
    "    \n",
    "    # Fill NaN values by replacing with the mean of the feature (axis=0)\n",
    "    for i in range(chunk.shape[2]):  # Iterate over features (columns)\n",
    "        col_mean = np.nanmean(chunk[:, :, i], axis=0)  # Compute mean of each feature\n",
    "        chunk[:, :, i] = np.nan_to_num(chunk[:, :, i], nan=col_mean)\n",
    "\n",
    "    # Append the processed chunk\n",
    "    X_train_chunks.append(chunk)\n",
    "\n",
    "# Process X_test in chunks (same approach)\n",
    "for start in range(0, X_test.shape[0], chunk_size):\n",
    "    end = min(start + chunk_size, X_test.shape[0])\n",
    "    \n",
    "    chunk = X_test[start:end]\n",
    "    \n",
    "    # Fill NaN values by replacing with the mean of the feature (axis=0)\n",
    "    for i in range(chunk.shape[2]):  # Iterate over features (columns)\n",
    "        col_mean = np.nanmean(chunk[:, :, i], axis=0)  # Compute mean of each feature\n",
    "        chunk[:, :, i] = np.nan_to_num(chunk[:, :, i], nan=col_mean)\n",
    "\n",
    "    # Append the processed chunk\n",
    "    X_test_chunks.append(chunk)\n",
    "\n",
    "# Concatenate all the chunks back together\n",
    "X_train = np.vstack(X_train_chunks)\n",
    "X_test = np.vstack(X_test_chunks)\n",
    "\n",
    "# Check if there are any NaN or Inf values left\n",
    "print(\"Checking for NaN in X_train after filling:\")\n",
    "print(np.isnan(X_train).sum(), np.isinf(X_train).sum())\n",
    "\n",
    "print(\"Checking for NaN in X_test after filling:\")\n",
    "print(np.isnan(X_test).sum(), np.isinf(X_test).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape before reshaping: (15261972, 1, 24)\n",
      "X_test shape before reshaping: (3815493, 1, 24)\n"
     ]
    }
   ],
   "source": [
    "# Check the current shape of X_train and X_test\n",
    "print(\"X_train shape before reshaping:\", X_train.shape)  # Expected: (15261972, 24)\n",
    "print(\"X_test shape before reshaping:\", X_test.shape)    # Expected: (3815493, 24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "476937/476937 [==============================] - 1367s 3ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n",
      "Epoch 2/10\n",
      "476937/476937 [==============================] - 2032s 4ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n",
      "Epoch 3/10\n",
      "476937/476937 [==============================] - 2136s 4ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n",
      "Epoch 4/10\n",
      "476937/476937 [==============================] - 2443s 5ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n",
      "Epoch 5/10\n",
      "476937/476937 [==============================] - 2478s 5ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n",
      "Epoch 6/10\n",
      "476937/476937 [==============================] - 2517s 5ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n",
      "Epoch 7/10\n",
      "476937/476937 [==============================] - 2517s 5ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n",
      "Epoch 8/10\n",
      "476937/476937 [==============================] - 2516s 5ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n",
      "Epoch 9/10\n",
      "476937/476937 [==============================] - 2519s 5ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n",
      "Epoch 10/10\n",
      "476937/476937 [==============================] - 2505s 5ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n"
     ]
    }
   ],
   "source": [
    "# Train the model directly without reshaping\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,  # Adjust number of epochs based on performance\n",
    "    batch_size=32,  # Adjust batch size if needed\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1  # Set to 1 to print training progress\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'timestamp' column to UNIX timestamp (seconds since epoch)\n",
    "if 'timestamp' in df.columns:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['timestamp'] = df['timestamp'].view('int64') // 10**9  # Convert to seconds since epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'q7257'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 6. Normalize the data\u001b[39;00m\n\u001b[0;32m     28\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m---> 29\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# 7. Reshape for LSTM (samples, time steps, features)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\palak\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\palak\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:916\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\palak\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:435\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\palak\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\palak\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:473\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinMaxScaler does not support sparse input. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using MaxAbsScaler instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    470\u001b[0m     )\n\u001b[0;32m    472\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 473\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m data_min \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    481\u001b[0m data_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\palak\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    603\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 605\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    607\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\palak\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py:836\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[0;32m    832\u001b[0m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[0;32m    834\u001b[0m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[0;32m    835\u001b[0m     new_dtype \u001b[38;5;241m=\u001b[39m dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[1;32m--> 836\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[0;32m    838\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\palak\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\generic.py:6324\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6317\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   6318\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[:, i]\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m   6319\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m   6320\u001b[0m     ]\n\u001b[0;32m   6322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6323\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6324\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6327\u001b[0m \u001b[38;5;66;03m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\palak\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\internals\\managers.py:451\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    449\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\palak\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\internals\\managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    355\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\palak\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:511\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;124;03mCoerce to the new dtype.\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;124;03mBlock\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    509\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m--> 511\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    513\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    515\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\palak\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:242\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    239\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 242\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\palak\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:187\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    184\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\palak\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:138\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m is_object_dtype(arr\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m is_object_dtype(dtype):\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'q7257'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 1. Load the dataset\n",
    "file_path = r\"C:\\Users\\palak\\Desktop\\sem6\\Intel_unaati\\zipped_processed_student_data_sampled.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 2. Convert 'timestamp' column to UNIX timestamp (if exists)\n",
    "if 'timestamp' in df.columns:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['timestamp'] = df['timestamp'].view('int64') // 10**9  # Convert to seconds since epoch\n",
    "\n",
    "# 3. One-hot encode categorical columns (if needed)\n",
    "df = pd.get_dummies(df, columns=['action_type', 'platform'], drop_first=True)\n",
    "\n",
    "# 4. Split the data into features (X) and target (y)\n",
    "X = df.drop(columns=['user_answer'])  # Replace 'user_answer' with your actual target column name\n",
    "y = df['user_answer']  # Replace 'user_answer' with your actual target column name\n",
    "\n",
    "# 5. Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 6. Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 7. Reshape for LSTM (samples, time steps, features)\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# 8. Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(1, X_train_lstm.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Binary Classification (0 or 1)\n",
    "])\n",
    "\n",
    "# 9. Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 10. Train the model\n",
    "history = model.fit(\n",
    "    X_train_lstm, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_lstm, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 11. Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_lstm, y_test, verbose=1)\n",
    "\n",
    "# 12. Print the results\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
