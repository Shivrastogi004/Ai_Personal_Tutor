{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 297915 CSV files. Processing 15% of them...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44687/44687 [6:14:13<00:00,  1.99it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Processing Complete! Saved to: C:\\Users\\palak\\Desktop\\sem6\\Intel_unaati\\zipped_processed_student_data_sampled.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# ‚úÖ Define Paths\n",
    "zip_file_path = r\"C:\\Users\\palak\\Desktop\\sem6\\Intel_unaati\\EdNet-KT4.zip\"\n",
    "output_file = r\"C:\\Users\\palak\\Desktop\\sem6\\Intel_unaati\\zipped_processed_student_data_sampled.csv\"\n",
    "\n",
    "# ‚úÖ Initialize an empty DataFrame\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# ‚úÖ Open ZIP file without extracting\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
    "    file_list = [f for f in z.namelist() if f.endswith('.csv')]  # Filter only CSV files\n",
    "\n",
    "    print(f\"‚úÖ Found {len(file_list)} CSV files. Processing 15% of them...\")\n",
    "\n",
    "    # Sample 25% of the files\n",
    "    sample_files = pd.Series(file_list).sample(frac=0.15, random_state=42).tolist()\n",
    "\n",
    "    for file in tqdm(sample_files, desc=\"Processing Files\"):\n",
    "        try:\n",
    "            # Read CSV directly from ZIP without extracting\n",
    "            with z.open(file) as f:\n",
    "                df = pd.read_csv(f)\n",
    "\n",
    "                # Convert timestamp column if it exists\n",
    "                if 'timestamp' in df.columns:\n",
    "                    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "\n",
    "                # Append to the combined dataframe\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping file {file} due to error: {e}\")\n",
    "\n",
    "# ‚úÖ Save the combined dataset\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ Data Processing Complete! Saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Loaded Successfully!\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19077465 entries, 0 to 19077464\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   timestamp    object \n",
      " 1   action_type  object \n",
      " 2   item_id      object \n",
      " 3   cursor_time  float64\n",
      " 4   source       object \n",
      " 5   user_answer  object \n",
      " 6   platform     object \n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 1018.8+ MB\n",
      "\n",
      "üìä Data Info:\n",
      " None\n",
      "\n",
      "üîç Sample Data:\n",
      "                  timestamp action_type item_id  cursor_time     source  \\\n",
      "0  2018-09-14 01:36:22.395       enter   b5060          NaN  diagnosis   \n",
      "1  2018-09-14 01:36:55.909     respond   q6528          NaN  diagnosis   \n",
      "2  2018-09-14 01:36:56.645      submit   b5060          NaN  diagnosis   \n",
      "3  2018-09-14 01:36:58.168       enter   b3779          NaN  diagnosis   \n",
      "4  2018-09-14 01:37:21.381     respond   q5247          NaN  diagnosis   \n",
      "\n",
      "  user_answer platform  \n",
      "0         NaN      web  \n",
      "1           d      web  \n",
      "2         NaN      web  \n",
      "3         NaN      web  \n",
      "4           b      web  \n",
      "\n",
      "‚ùå Missing Values:\n",
      " timestamp             0\n",
      "action_type           0\n",
      "item_id               0\n",
      "cursor_time    13731578\n",
      "source             4260\n",
      "user_answer    14982799\n",
      "platform           4260\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Load Processed Data\n",
    "file_path = r\"C:\\Users\\palak\\Desktop\\sem6\\Intel_unaati\\zipped_processed_student_data_sampled.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ‚úÖ Display Data Info\n",
    "print(\"‚úÖ Data Loaded Successfully!\")\n",
    "print(\"\\nüìä Data Info:\\n\", df.info())\n",
    "print(\"\\nüîç Sample Data:\\n\", df.head())\n",
    "\n",
    "print(\"\\n‚ùå Missing Values:\\n\", df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for numerical columns\n",
    "df.fillna(method='ffill', inplace=True)  # Forward fill\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime and extract features\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.hour  # Extract hour\n",
    "df['day'] = df['timestamp'].dt.day  # Extract day\n",
    "df['month'] = df['timestamp'].dt.month  # Extract month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical features\n",
    "df = pd.get_dummies(df, columns=['action_type', 'source', 'platform'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train LSTM for Student Engagement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 timestamp action_type item_id  cursor_time     source  \\\n",
      "0  2018-09-14 01:36:22.395       enter   b5060          NaN  diagnosis   \n",
      "1  2018-09-14 01:36:55.909     respond   q6528          NaN  diagnosis   \n",
      "2  2018-09-14 01:36:56.645      submit   b5060          NaN  diagnosis   \n",
      "3  2018-09-14 01:36:58.168       enter   b3779          NaN  diagnosis   \n",
      "4  2018-09-14 01:37:21.381     respond   q5247          NaN  diagnosis   \n",
      "\n",
      "  user_answer platform  \n",
      "0         NaN      web  \n",
      "1           d      web  \n",
      "2         NaN      web  \n",
      "3         NaN      web  \n",
      "4           b      web  \n",
      "Index(['timestamp', 'action_type', 'item_id', 'cursor_time', 'source',\n",
      "       'user_answer', 'platform'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\palak\\Desktop\\sem6\\Intel_unaati\\zipped_processed_student_data_sampled.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display first few rows to understand its structure\n",
    "print(df.head())\n",
    "\n",
    "# Display column names to ensure all features are present\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['action_type', 'item_id', 'cursor_time', 'source', 'user_answer',\n",
      "       'platform', 'hour', 'day', 'month'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check the actual columns in the DataFrame\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19077465 entries, 0 to 19077464\n",
      "Data columns (total 26 columns):\n",
      " #   Column                         Dtype  \n",
      "---  ------                         -----  \n",
      " 0   item_id                        object \n",
      " 1   cursor_time                    float64\n",
      " 2   user_answer                    object \n",
      " 3   hour                           int32  \n",
      " 4   day                            int32  \n",
      " 5   month                          int32  \n",
      " 6   action_type_enter              bool   \n",
      " 7   action_type_erase_choice       bool   \n",
      " 8   action_type_pause_audio        bool   \n",
      " 9   action_type_pause_video        bool   \n",
      " 10  action_type_pay                bool   \n",
      " 11  action_type_play_audio         bool   \n",
      " 12  action_type_play_video         bool   \n",
      " 13  action_type_quit               bool   \n",
      " 14  action_type_refund             bool   \n",
      " 15  action_type_respond            bool   \n",
      " 16  action_type_submit             bool   \n",
      " 17  action_type_undo_erase_choice  bool   \n",
      " 18  source_archive                 bool   \n",
      " 19  source_diagnosis               bool   \n",
      " 20  source_my_note                 bool   \n",
      " 21  source_review                  bool   \n",
      " 22  source_review_quiz             bool   \n",
      " 23  source_sprint                  bool   \n",
      " 24  source_tutor                   bool   \n",
      " 25  platform_web                   bool   \n",
      "dtypes: bool(20), float64(1), int32(3), object(2)\n",
      "memory usage: 1018.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\palak\\Desktop\\sem6\\Intel_unaati\\zipped_processed_student_data_sampled.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'timestamp' column to datetime and extract features\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['day'] = df['timestamp'].dt.day\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "\n",
    "# Drop 'timestamp' column as it's no longer needed\n",
    "df.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "# One-hot encode categorical features (action_type, source, platform)\n",
    "df = pd.get_dummies(df, columns=['action_type', 'source', 'platform'], drop_first=True)\n",
    "\n",
    "# Fill missing values by forward filling\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Check the data after preprocessing\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\palak\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:480: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "c:\\Users\\palak\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:481: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15261972, 1, 24) (3815493, 1, 24)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the chunk size (adjust as needed)\n",
    "chunk_size = 10000  # You can reduce this if memory is still an issue\n",
    "\n",
    "# Initialize a list to store processed chunks\n",
    "chunks = []\n",
    "\n",
    "# Initialize LabelEncoder for user_answer\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Iterate over the CSV file in chunks\n",
    "for chunk in pd.read_csv(r\"C:\\Users\\palak\\Desktop\\sem6\\Intel_unaati\\zipped_processed_student_data_sampled.csv\", \n",
    "                         chunksize=chunk_size):\n",
    "    \n",
    "    # Process each chunk: Drop 'item_id' column\n",
    "    chunk.drop(columns=['item_id'], inplace=True)\n",
    "    \n",
    "    # Convert 'user_answer' to numeric labels\n",
    "    chunk['user_answer'] = label_encoder.fit_transform(chunk['user_answer'])\n",
    "    \n",
    "    # Extract time-related features from 'timestamp'\n",
    "    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])\n",
    "    chunk['hour'] = chunk['timestamp'].dt.hour\n",
    "    chunk['day'] = chunk['timestamp'].dt.day\n",
    "    chunk['month'] = chunk['timestamp'].dt.month\n",
    "    \n",
    "    # Drop 'timestamp' as it's no longer needed\n",
    "    chunk.drop(columns=['timestamp'], inplace=True)\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    chunk = pd.get_dummies(chunk, columns=['action_type', 'source', 'platform'], drop_first=True)\n",
    "    \n",
    "    # Fill missing values by forward filling\n",
    "    chunk.fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    # Scale the features\n",
    "    X = chunk.drop(columns=['user_answer'])\n",
    "    y = chunk['user_answer']\n",
    "    X_scaled = scaler.fit_transform(X)  # Scale the chunk\n",
    "\n",
    "    # Add the scaled chunk to the list\n",
    "    chunks.append((X_scaled, y))\n",
    "\n",
    "# Combine all the chunks\n",
    "X_combined = pd.concat([pd.DataFrame(chunk[0]) for chunk in chunks], axis=0)\n",
    "y_combined = pd.concat([pd.Series(chunk[1]) for chunk in chunks], axis=0)\n",
    "\n",
    "# Reshape for LSTM (samples, time_steps, features)\n",
    "X_lstm = X_combined.values.reshape((X_combined.shape[0], 1, X_combined.shape[1]))  # Adding a time step dimension\n",
    "\n",
    "# Split into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lstm, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of the data\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15261972, 1, 24) (3815493, 1, 24)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define chunk size for memory efficiency\n",
    "chunk_size = 100000\n",
    "\n",
    "# Initialize a list to store processed chunks\n",
    "chunks = []\n",
    "\n",
    "# Initialize LabelEncoder and MinMaxScaler\n",
    "label_encoder = LabelEncoder()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Iterate over the CSV file in chunks\n",
    "for chunk in pd.read_csv(r\"C:\\Users\\palak\\Desktop\\sem6\\Intel_unaati\\zipped_processed_student_data_sampled.csv\", \n",
    "                         chunksize=chunk_size):\n",
    "    \n",
    "    # Drop 'item_id' column as it's not needed for training\n",
    "    chunk.drop(columns=['item_id'], inplace=True)\n",
    "    \n",
    "    # Convert 'user_answer' to numeric labels\n",
    "    chunk['user_answer'] = label_encoder.fit_transform(chunk['user_answer'])\n",
    "    \n",
    "    # Extract time-related features from 'timestamp'\n",
    "    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])\n",
    "    chunk['hour'] = chunk['timestamp'].dt.hour\n",
    "    chunk['day'] = chunk['timestamp'].dt.day\n",
    "    chunk['month'] = chunk['timestamp'].dt.month\n",
    "    \n",
    "    # Drop 'timestamp' as it's no longer needed\n",
    "    chunk.drop(columns=['timestamp'], inplace=True)\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    chunk = pd.get_dummies(chunk, columns=['action_type', 'source', 'platform'], drop_first=True)\n",
    "    \n",
    "    # Fill missing values by forward filling\n",
    "    chunk.fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    # Remove columns that are completely NaN (if any)\n",
    "    chunk = chunk.dropna(axis=1, how='all')\n",
    "    \n",
    "    # Scale the features\n",
    "    X = chunk.drop(columns=['user_answer'])\n",
    "    y = chunk['user_answer']\n",
    "    X_scaled = scaler.fit_transform(X)  # Scale the chunk\n",
    "\n",
    "    # Add the scaled chunk to the list\n",
    "    chunks.append((X_scaled, y))\n",
    "\n",
    "# Combine all the chunks\n",
    "X_combined = pd.concat([pd.DataFrame(chunk[0]) for chunk in chunks], axis=0)\n",
    "y_combined = pd.concat([pd.Series(chunk[1]) for chunk in chunks], axis=0)\n",
    "\n",
    "# Reshape for LSTM (samples, time_steps, features)\n",
    "X_lstm = X_combined.values.reshape((X_combined.shape[0], 1, X_combined.shape[1]))  # Adding a time step dimension\n",
    "\n",
    "# Split into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lstm, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of the data\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 1, 64)             22784     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1, 64)             0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35299 (137.89 KB)\n",
      "Trainable params: 35299 (137.89 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),  # Shape (samples, time_steps, features)\n",
    "    Dropout(0.2),  # Dropout to prevent overfitting\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(3, activation='softmax')  # 3 classes: 'user_answer_b', 'user_answer_c', 'user_answer_d'\n",
    "])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'user_answer': [4 3 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Check the unique values in 'user_answer'\n",
    "print(\"Unique values in 'user_answer':\", y_combined.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values after re-encoding: [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Re-encode 'user_answer' to correctly reflect 5 classes\n",
    "label_encoder = LabelEncoder()\n",
    "y_combined = label_encoder.fit_transform(y_combined)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Check the unique values after re-encoding\n",
    "print(\"Unique values after re-encoding:\", np.unique(y_combined))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 1, 64)             22784     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1, 64)             0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35365 (138.14 KB)\n",
      "Trainable params: 35365 (138.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Rebuild the LSTM model with correct number of output units\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),  # Shape (samples, time_steps, features)\n",
    "    Dropout(0.2),  # Dropout to prevent overfitting\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(5, activation='softmax')  # Update to 5 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary to confirm changes\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaN or Inf in X_train:\n",
      "16384934 0\n",
      "Checking for NaN or Inf in y_train:\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any NaN or infinite values in the dataset\n",
    "import numpy as np\n",
    "\n",
    "print(\"Checking for NaN or Inf in X_train:\")\n",
    "print(np.isnan(X_train).sum(), np.isinf(X_train).sum())\n",
    "\n",
    "print(\"Checking for NaN or Inf in y_train:\")\n",
    "print(np.isnan(y_train).sum(), np.isinf(y_train).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaN in X_train after filling:\n",
      "0 0\n",
      "Checking for NaN in X_test after filling:\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define chunk size (adjust based on available memory)\n",
    "chunk_size = 100000  # Adjust the chunk size\n",
    "\n",
    "# Initialize empty lists to store processed chunks\n",
    "X_train_chunks = []\n",
    "X_test_chunks = []\n",
    "\n",
    "# Process X_train in chunks\n",
    "for start in range(0, X_train.shape[0], chunk_size):\n",
    "    end = min(start + chunk_size, X_train.shape[0])\n",
    "    \n",
    "    chunk = X_train[start:end]\n",
    "    \n",
    "    # Fill NaN values by replacing with the mean of the feature (axis=0)\n",
    "    for i in range(chunk.shape[2]):  # Iterate over features (columns)\n",
    "        col_mean = np.nanmean(chunk[:, :, i], axis=0)  # Compute mean of each feature\n",
    "        chunk[:, :, i] = np.nan_to_num(chunk[:, :, i], nan=col_mean)\n",
    "\n",
    "    # Append the processed chunk\n",
    "    X_train_chunks.append(chunk)\n",
    "\n",
    "# Process X_test in chunks (same approach)\n",
    "for start in range(0, X_test.shape[0], chunk_size):\n",
    "    end = min(start + chunk_size, X_test.shape[0])\n",
    "    \n",
    "    chunk = X_test[start:end]\n",
    "    \n",
    "    # Fill NaN values by replacing with the mean of the feature (axis=0)\n",
    "    for i in range(chunk.shape[2]):  # Iterate over features (columns)\n",
    "        col_mean = np.nanmean(chunk[:, :, i], axis=0)  # Compute mean of each feature\n",
    "        chunk[:, :, i] = np.nan_to_num(chunk[:, :, i], nan=col_mean)\n",
    "\n",
    "    # Append the processed chunk\n",
    "    X_test_chunks.append(chunk)\n",
    "\n",
    "# Concatenate all the chunks back together\n",
    "X_train = np.vstack(X_train_chunks)\n",
    "X_test = np.vstack(X_test_chunks)\n",
    "\n",
    "# Check if there are any NaN or Inf values left\n",
    "print(\"Checking for NaN in X_train after filling:\")\n",
    "print(np.isnan(X_train).sum(), np.isinf(X_train).sum())\n",
    "\n",
    "print(\"Checking for NaN in X_test after filling:\")\n",
    "print(np.isnan(X_test).sum(), np.isinf(X_test).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape before reshaping: (15261972, 1, 24)\n",
      "X_test shape before reshaping: (3815493, 1, 24)\n"
     ]
    }
   ],
   "source": [
    "# Check the current shape of X_train and X_test\n",
    "print(\"X_train shape before reshaping:\", X_train.shape)  # Expected: (15261972, 24)\n",
    "print(\"X_test shape before reshaping:\", X_test.shape)    # Expected: (3815493, 24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "476937/476937 [==============================] - 1367s 3ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n",
      "Epoch 2/10\n",
      "476937/476937 [==============================] - 2032s 4ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n",
      "Epoch 3/10\n",
      "476937/476937 [==============================] - 2136s 4ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n",
      "Epoch 4/10\n",
      "476937/476937 [==============================] - 2443s 5ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n",
      "Epoch 5/10\n",
      "476937/476937 [==============================] - 2478s 5ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n",
      "Epoch 6/10\n",
      "476937/476937 [==============================] - 2517s 5ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n",
      "Epoch 7/10\n",
      "476937/476937 [==============================] - 2517s 5ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n",
      "Epoch 8/10\n",
      "476937/476937 [==============================] - 2516s 5ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n",
      "Epoch 9/10\n",
      "476937/476937 [==============================] - 2519s 5ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n",
      "Epoch 10/10\n",
      "476937/476937 [==============================] - 2505s 5ms/step - loss: nan - accuracy: 0.0607 - val_loss: nan - val_accuracy: 0.0607\n"
     ]
    }
   ],
   "source": [
    "# Train the model directly without reshaping\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,  # Adjust number of epochs based on performance\n",
    "    batch_size=32,  # Adjust batch size if needed\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1  # Set to 1 to print training progress\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'timestamp' column to UNIX timestamp (seconds since epoch)\n",
    "if 'timestamp' in df.columns:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['timestamp'] = df['timestamp'].view('int64') // 10**9  # Convert to seconds since epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
